{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "be2210bd-bfa4-4a19-9896-62d255ee4006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\omen\\anaconda3\\lib\\site-packages (2.21.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: evaluate in c:\\users\\omen\\anaconda3\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\omen\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\omen\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\omen\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\omen\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\omen\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets evaluate transformers rouge-score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f47cd23d-7996-475b-9d05-afe524b68c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "500b182f-bf90-4d69-85b7-c236b9792053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a1704f46-9b99-4b43-a0f2-f84563e2e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"summarization_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "98787a00-afea-4eee-a48f-0f6e656c133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "74db98a5-3af3-4b3c-b76b-1d17e78a0bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1671b8b9-fc53-4b55-b106-d4f3a8f89c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cec90c26-cc23-4de9-9d81-7d64eeda355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n",
       " 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.',\n",
       " 'id': '35232142'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354d398-f896-49f8-9262-0eead6c00a9f",
   "metadata": {},
   "source": [
    "### Random Samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a308ee20-6d1b-4be4-9bdd-476cd41c028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1cf7678e-0b9c-4691-9ad1-aa487d3f92bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Private Harry Vasey, who was part of the 1st Airborne Battalion, The Border Regiment, was killed during Operation Market Garden in Oosterbeek in 1944.\\nNow his identity has been confirmed, the Ministry of Defence (MoD) want to trace his family so his grave can be rededicated in the Netherlands.\\nThe MoD said plans were also in place to change his headstone.\\nBorn in Durham in May 1916 to Harry Vasey and Annie Young, he enlisted in April 1940 when he lived in Bowburn, County Durham.\\nAn MoD spokesman said: \"Unfortunately that is about all we know about Private Vasey and his family and that's where the trail goes cold.\\n\"We are hoping that there are some of his family still living in that area.\"\\nSince WW2, a section of the Royal Netherlands Army has been working to identify the graves of unknown soldiers killed in battle.\\nThe exhumation reports were scrutinised for clues to the identities of these men and the research was presented to the MoD.\\nMr Vasey is one of six Border Regiment soldiers, including Lance Corporal Raymond Halliday, to be indentified.\\nThe aim of Operation Market Garden was to take strategic bridges near Arnhem, but the Allies underestimated the number of German troops lying in wait and it failed.\\nMore than 1,400 Allied troops died and more than 6,000 were captured by German forces.\\nIt is hoped Mr Vasey's surviving relatives can attend the service at Oosterbeek Cemetery on 14 September in honour of his sacrifice and bravery, the MoD said.</td>\n",
       "      <td>The family of a soldier killed during World War Two is being sought after his final resting place was confirmed.</td>\n",
       "      <td>36802710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hooker Richard Hibbard and prop John Afoa return as David Humphreys makes 10 changes to the team which beat the French side 35-14 on 8 December.\\nLewis Ludlow, Jacob Rowan and Ben Morgan also start in the pack, while Callum Braley starts at scrum-half.\\nJames Hook takes David Halaifonua's spot at full-back, with Jonny May and Charlie Sharples on the wings.\\nStade Rochelais: Murimurivalu; Lacroix, Jordaan, Aguillon, Rattez; James, Bales; Corbel, Forbes, Atonio, Qovu, Tanguy, Sazy, Eaton (capt.), Vito.\\nReplacements: Maurouard, Priso, Boughanmi, Cedaro, Francoz, Retiere, Holmes, Botia.\\nGloucester: Hook; Sharples, Scott, Atkinson, May; Burns, Braley (capt.); Hohneck, Hibbard, Afoa, Savage, Thrush, Ludlow, Rowan, Morgan.\\nReplacements: Matu'u, Thomas, Doran-Jonesm Latta, Galarza, Heinz, Purdy, Kvesic.\\nFor the latest rugby union news follow @bbcrugbyunion on Twitter.</td>\n",
       "      <td>Gloucester lock Jeremy Thrush will make his first appearance of the season against Stade Rochelais.</td>\n",
       "      <td>38307773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The university announced back in March it needed to save £10.5m and planned to cut 150 posts.\\nMembers of the University and College Union (UCU) Scotland were balloted as a result.\\nAfter the ballot closed on Monday, the union said 73% of those who voted backed strike action. And 80% also voted for action short of a strike.\\nAndrew MacKillop, Aberdeen UCU representative, said: \"Members have made it quite clear that they reject the job losses proposed by the university.\\n\"Strike action is always a last resort but we can't sit back and see jobs lost with the accompanying damage to the student experience and the reputation of the university.\"\\nIn a statement, the university said it was disappointed that the union had vote for strike action \"in the midst of ongoing dialogue\".\\nIt added: \"According to the results of the ballot, 263 UCU members voted in favour of strike action, representing 12.5% of our total academic and academic-related workforce.\\n\"The UCU had asked for assurance that the university would rule out compulsory redundancies as it seeks to make savings of £10.5m.\\n\"We were unable to give that assurance, although we are working tirelessly to achieve the savings we need through voluntary measures as far as possible, and are pursuing a range of additional options to increase our efficiency as a world-leading university.\"</td>\n",
       "      <td>Staff at the University of Aberdeen have backed plans for industrial action in a dispute over planned job losses.</td>\n",
       "      <td>33049482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A selection of your pictures of Scotland sent in between 18 and 25 August. Send your photos to scotlandpictures@bbc.co.uk or via Instagram at #bbcscotlandpics</td>\n",
       "      <td>All images are copyrighted.</td>\n",
       "      <td>41047208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phillip Fawcett, 48, from Lancaster had worked as a piano teacher for 25 years, teaching pupils in their own homes.\\nHe admitted at Preston Crown Court to storing hundreds of child abuse images and posting on a paedophile website.\\nThe married father-of-two was given a three-year community order and banned from working with children.\\nSentencing, Recorder Guy Mathieson told Fawcett he had written a number of \"sickening articles, extolling the virtues of paedophilia\".\\n\"You delighted, it seems, in discussing your desires and expressing your desires in this way on the internet with other likeminded individuals, with whom you discussed your depravity, legitimising, encouraging and involving this behaviour in others and yourself - and the abuse of children.\\n\"Part of the horror of this case is that whilst you were leading this secret life, revelling in the delights of abusing children, you were going into people's homes to teach their children music.\"\\nLancashire Police officers raided Fawcett's home on 28 January seizing a laptop and a number of memory sticks containing hundreds of prohibited images of children aged between five and 15.\\nThe court heard he used the images as inspiration for the stories and poems of around 1,500 words each that he uploaded to a paedophile's website.\\nFollowing his arrest, Fawcett admitted he had been using the website since 2012 and had been sexually attracted to children for the last decade.\\nFawcett pleaded guilty to three counts of making indecent images, one of possessing indecent images, one of possessing prohibited images and six counts of publishing obscene material.\\nHe was given a three-year community order, ordered to complete a 60 day sex offenders' rehabilitation program and must sign the sex offenders' register for five years.</td>\n",
       "      <td>A piano teacher who uploaded \"sickening\" stories about child abuse after viewing indecent images of children has been sentenced.</td>\n",
       "      <td>38146988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "457650aa-293a-4657-ac5b-e75e987019e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each prediction\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLsum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_aggregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (f1),\n",
       "    rouge2: rouge_2 (f1),\n",
       "    rougeL: rouge_l (f1),\n",
       "    rougeLsum: rouge_lsum (f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = evaluate.load('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "18169a0c-9003-4bc3-8d15-cf90e0080a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6339bb31-fe15-4afb-b649-f9866b75a842",
   "metadata": {},
   "source": [
    "### Tokenization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "951a699f-b967-4b62-8e30-904cf6eb439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "23f5daeb-e8c8-4d81-81ac-b2a17d960bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [8774, 6, 48, 80, 7142, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "92c69774-506f-4117-9d56-ef7ab7c45721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "38fade35-a268-41b7-82eb-9e4139f60f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(text_target=[\"Hello, this one sentence!\", \"This is another sentence.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "41fc45a8-45fd-476f-84ea-5b6ecc5099fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a9f8ba23-5ed0-4c4f-bd98-e75e42fb0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "32559dd0-057a-4bd0-b141-7498f747d90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[21603, 10, 37, 423, 583, 13, 1783, 16, 20126, 16496, 6, 80, 13, 8, 844, 6025, 4161, 6, 19, 341, 271, 14841, 5, 7057, 161, 19, 4912, 16, 1626, 5981, 11, 186, 7540, 16, 1276, 15, 2296, 7, 5718, 2367, 14621, 4161, 57, 4125, 387, 5, 15059, 7, 30, 8, 4653, 4939, 711, 747, 522, 17879, 788, 12, 1783, 44, 8, 15763, 6029, 1813, 9, 7472, 5, 1404, 1623, 11, 5699, 277, 130, 4161, 57, 18368, 16, 20126, 16496, 227, 8, 2473, 5895, 15, 147, 89, 22411, 139, 8, 1511, 5, 1485, 3271, 3, 21926, 9, 472, 19623, 5251, 8, 616, 12, 15614, 8, 1783, 5, 37, 13818, 10564, 15, 26, 3, 9, 3, 19513, 1481, 6, 18368, 186, 1328, 2605, 30, 7488, 1887, 3, 18, 8, 711, 2309, 9517, 89, 355, 5, 3966, 1954, 9233, 15, 6, 113, 293, 7, 8, 16548, 13363, 106, 14022, 84, 47, 14621, 4161, 6, 243, 255, 228, 59, 7828, 8, 1249, 18, 545, 11298, 1773, 728, 8, 8347, 1560, 5, 611, 6, 255, 243, 72, 1709, 1528, 161, 228, 43, 118, 4006, 91, 12, 766, 8, 3, 19513, 1481, 410, 59, 5124, 5, 96, 196, 17, 19, 1256, 68, 27, 103, 317, 132, 19, 78, 231, 23546, 21, 970, 51, 89, 2593, 11, 8, 2504, 189, 3, 18, 11, 27, 3536, 3653, 24, 3, 18, 68, 34, 19, 966, 114, 62, 31, 60, 23708, 42, 11821, 976, 255, 243, 5, 96, 11880, 164, 59, 36, 1176, 68, 34, 19, 2361, 82, 3503, 147, 8, 336, 360, 477, 5, 96, 17891, 130, 25, 59, 1065, 12, 199, 178, 3, 9, 720, 72, 116, 8, 6337, 11, 8, 6196, 5685, 7, 141, 2767, 91, 4609, 7940, 6, 3, 9, 8347, 5685, 3048, 16, 286, 640, 8, 17600, 7, 250, 13, 8, 3917, 3412, 5, 1276, 15, 2296, 7, 47, 14621, 1560, 57, 982, 6, 13233, 53, 3088, 12, 4277, 72, 13613, 7, 16, 8, 616, 5, 12580, 17600, 7, 2063, 65, 474, 3, 9, 570, 30, 165, 475, 13, 8, 7540, 6025, 4161, 11, 3863, 43, 118, 3, 19492, 59, 12, 9751, 12493, 3957, 5, 37, 16117, 3450, 31, 7, 21108, 12580, 2488, 5104, 11768, 1306, 47, 16, 1626, 5981, 30, 2089, 12, 217, 8, 1419, 166, 609, 5, 216, 243, 34, 47, 359, 12, 129, 8, 8347, 1711, 515, 269, 68, 3, 9485, 3088, 12, 1634, 95, 8, 433, 5, 96, 196, 47, 882, 1026, 3, 9, 1549, 57, 8, 866, 13, 1783, 24, 65, 118, 612, 976, 3, 88, 243, 5, 96, 14116, 34, 19, 842, 18, 18087, 21, 151, 113, 43, 118, 5241, 91, 13, 70, 2503, 11, 8, 1113, 30, 1623, 535, 216, 243, 34, 47, 359, 24, 96, 603, 5700, 342, 2245, 121, 130, 1026, 12, 1822, 8, 844, 167, 9930, 11, 3, 9, 964, 97, 3869, 474, 16, 286, 21, 8347, 9793, 1390, 5, 2114, 25, 118, 4161, 57, 18368, 16, 970, 51, 89, 2593, 11, 10987, 32, 1343, 42, 8, 17600, 7, 58, 8779, 178, 81, 39, 351, 13, 8, 1419, 11, 149, 34, 47, 10298, 5, 8601, 178, 30, 142, 40, 157, 12546, 5, 15808, 1741, 115, 115, 75, 5, 509, 5, 1598, 42, 146, 51, 89, 2593, 1741, 115, 115, 75, 5, 509, 5, 1598, 5, 1], [21603, 10, 71, 1472, 6196, 877, 326, 44, 8, 9108, 86, 29, 16, 6000, 1887, 44, 81, 11484, 10, 1755, 272, 4209, 30, 1856, 11, 2554, 130, 1380, 12, 1175, 8, 1595, 5, 282, 79, 3, 9094, 1067, 79, 1509, 8, 192, 14264, 6, 3, 16669, 596, 18, 969, 18, 1583, 16, 8, 443, 2447, 6, 3, 35, 6106, 19565, 57, 12314, 7, 5, 555, 13, 8, 1552, 1637, 19, 45, 3434, 6, 8, 119, 45, 1473, 11, 14441, 5, 94, 47, 70, 166, 706, 16, 5961, 5316, 5, 37, 2535, 13, 80, 13, 8, 14264, 243, 186, 13, 8, 9234, 141, 646, 525, 12770, 7, 30, 1476, 11, 175, 141, 118, 10932, 5, 2867, 1637, 43, 13666, 3709, 11210, 11, 56, 1731, 70, 1552, 13, 8, 3457, 4939, 865, 145, 79, 141, 4355, 5, 5076, 43, 3958, 15, 26, 21, 251, 81, 8, 3211, 5, 86, 7, 102, 1955, 24723, 243, 10, 96, 196, 17, 3475, 38, 713, 8, 1472, 708, 365, 80, 13, 8, 14264, 274, 16436, 12, 8, 511, 5, 96, 27674, 8, 2883, 1137, 19, 341, 365, 4962, 6, 34, 19, 816, 24, 8, 1472, 47, 708, 24067, 535, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7433, 18, 413, 2673, 33, 6168, 640, 8, 12580, 17600, 7, 11, 970, 51, 89, 2593, 11, 10987, 32, 1343, 227, 18368, 2953, 57, 16133, 4937, 5, 1], [2759, 8548, 14264, 43, 118, 10932, 57, 1472, 16, 3, 9, 18024, 1584, 739, 3211, 16, 27874, 690, 2050, 5, 1]]}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6f988c48-3c1a-48e4-a767-127483d799b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e699b8c3-0096-4055-af4b-d6a15bd264b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in c:\\users\\omen\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\omen\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\omen\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\omen\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6350eb2d-8e9c-49a2-896e-1877f0594f15",
   "metadata": {},
   "source": [
    "### Model Setup and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "52600303-83ba-4189-b5b3-12a15ba820bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "211e968d-f129-4eb8-8cb7-02bb3c334714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) # Google Colab (GPU)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "daeb8b51-11b3-4906-bb01-d7cc08742c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\omen\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\omen\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\omen\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\omen\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "408e5bd2-0b94-4c9f-b67d-4f685440c292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\omen\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.5.1+cu124)\n",
      "Requirement already satisfied: numpy in c:\\users\\omen\\anaconda3\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: typing_extensions>=4.8.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Downloading bitsandbytes-0.45.0-py3-none-win_amd64.whl (68.5 MB)\n",
      "   ---------------------------------------- 0.0/68.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/68.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.8/68.5 MB 2.8 MB/s eta 0:00:25\n",
      "    --------------------------------------- 1.3/68.5 MB 2.6 MB/s eta 0:00:27\n",
      "   - -------------------------------------- 1.8/68.5 MB 2.6 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 2.4/68.5 MB 2.6 MB/s eta 0:00:26\n",
      "   - -------------------------------------- 2.9/68.5 MB 2.6 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 3.7/68.5 MB 2.7 MB/s eta 0:00:24\n",
      "   -- ------------------------------------- 4.5/68.5 MB 2.9 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 5.2/68.5 MB 2.9 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 6.0/68.5 MB 3.0 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 6.6/68.5 MB 3.0 MB/s eta 0:00:21\n",
      "   ---- ----------------------------------- 7.6/68.5 MB 3.2 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 8.1/68.5 MB 3.1 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 8.7/68.5 MB 3.1 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 9.4/68.5 MB 3.1 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 10.2/68.5 MB 3.2 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 10.7/68.5 MB 3.2 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 11.3/68.5 MB 3.2 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 12.1/68.5 MB 3.2 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 12.8/68.5 MB 3.2 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 13.1/68.5 MB 3.1 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 13.9/68.5 MB 3.1 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 14.4/68.5 MB 3.1 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 15.2/68.5 MB 3.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 15.5/68.5 MB 3.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 15.5/68.5 MB 3.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 15.7/68.5 MB 2.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 16.3/68.5 MB 2.9 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 16.5/68.5 MB 2.8 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 16.8/68.5 MB 2.8 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 17.0/68.5 MB 2.7 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 17.3/68.5 MB 2.7 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 17.6/68.5 MB 2.6 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 17.8/68.5 MB 2.6 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 18.1/68.5 MB 2.6 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 18.6/68.5 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 18.9/68.5 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 19.1/68.5 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 19.7/68.5 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 19.9/68.5 MB 2.5 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 20.2/68.5 MB 2.4 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 20.4/68.5 MB 2.4 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 21.0/68.5 MB 2.4 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 21.2/68.5 MB 2.4 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 21.8/68.5 MB 2.4 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 22.0/68.5 MB 2.4 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 22.5/68.5 MB 2.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 22.8/68.5 MB 2.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 23.3/68.5 MB 2.3 MB/s eta 0:00:20\n",
      "   ------------- -------------------------- 23.9/68.5 MB 2.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 24.1/68.5 MB 2.3 MB/s eta 0:00:20\n",
      "   -------------- ------------------------- 24.6/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 25.2/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 25.4/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   -------------- ------------------------- 25.7/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 26.2/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 26.5/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 26.7/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   --------------- ------------------------ 27.3/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 27.8/68.5 MB 2.3 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 28.0/68.5 MB 2.3 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 28.3/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 28.6/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 28.8/68.5 MB 2.2 MB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 29.1/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 29.6/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 30.1/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 30.4/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 30.7/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 30.7/68.5 MB 2.2 MB/s eta 0:00:18\n",
      "   ------------------ --------------------- 31.7/68.5 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 32.2/68.5 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------ --------------------- 32.5/68.5 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 33.0/68.5 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 33.3/68.5 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 33.6/68.5 MB 2.2 MB/s eta 0:00:17\n",
      "   ------------------- -------------------- 34.1/68.5 MB 2.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 34.3/68.5 MB 2.1 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 34.6/68.5 MB 2.1 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 34.9/68.5 MB 2.1 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 35.4/68.5 MB 2.1 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 35.9/68.5 MB 2.1 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 36.4/68.5 MB 2.1 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 36.7/68.5 MB 2.1 MB/s eta 0:00:15\n",
      "   --------------------- ------------------ 37.2/68.5 MB 2.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 37.7/68.5 MB 2.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 38.0/68.5 MB 2.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 38.5/68.5 MB 2.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 39.1/68.5 MB 2.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 39.6/68.5 MB 2.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 40.1/68.5 MB 2.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 40.4/68.5 MB 2.1 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 40.9/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 41.4/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 41.9/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 42.2/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 42.5/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 42.7/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 43.0/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 43.3/68.5 MB 2.1 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 43.5/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 43.8/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 44.0/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 44.6/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 44.8/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 44.8/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 45.6/68.5 MB 2.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 46.1/68.5 MB 2.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 46.7/68.5 MB 2.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 46.9/68.5 MB 2.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 47.4/68.5 MB 2.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 47.7/68.5 MB 2.1 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 48.0/68.5 MB 2.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 48.0/68.5 MB 2.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 49.0/68.5 MB 2.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 49.5/68.5 MB 2.1 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 49.8/68.5 MB 2.1 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 50.3/68.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 50.9/68.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 51.4/68.5 MB 2.1 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 51.9/68.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 52.7/68.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 53.2/68.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 53.7/68.5 MB 2.1 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 54.5/68.5 MB 2.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 55.1/68.5 MB 2.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 55.6/68.5 MB 2.1 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 56.1/68.5 MB 2.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 56.6/68.5 MB 2.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 57.1/68.5 MB 2.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 57.4/68.5 MB 2.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 57.7/68.5 MB 2.1 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 58.2/68.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 58.5/68.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 58.5/68.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 59.0/68.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 59.5/68.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 59.8/68.5 MB 2.1 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 60.3/68.5 MB 2.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 60.8/68.5 MB 2.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 61.3/68.5 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 62.1/68.5 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 62.7/68.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 63.2/68.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 63.7/68.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 64.2/68.5 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 64.7/68.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 65.3/68.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 65.8/68.5 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 66.3/68.5 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  66.8/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  67.4/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  67.6/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  68.2/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  68.4/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  68.4/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  68.4/68.5 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 68.5/68.5 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd730e26-e286-4aab-b8cd-aa76baf9b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model in Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model_dir = \"/content/drive/My Drive/Models/t5-summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117894ab-a695-4f9e-8ed9-b7c1dde1ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # Save to Google Drive\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[\"none\"],\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    # save_steps=500,  # Save checkpoint every 500 steps\n",
    "    # logging_dir='./logs',  # Directory to store logs\n",
    "    # logging_steps=100  # Log metrics every 100 steps\n",
    "    # use_cpu=True,  # Force CPU usage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c800d83b-326a-470b-a9a7-085b232dd469",
   "metadata": {},
   "source": [
    "### Evaluation Metric (ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e4bee2dd-105b-4b96-a295-86437ab2cd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "233d7002-ed74-40eb-b48a-ddde258db53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    # Note that other metrics may not have a `use_aggregator` parameter\n",
    "    # and thus will return a list, computing a metric for each sentence.\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ff5a7-f092-4c00-8703-732e9e654eb1",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537827bb-0ca1-4199-a369-4fe83ef3596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e84f7-6913-44a9-9777-33edfa4b37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a6171-475f-48ec-865f-bf079217f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce356a-aebd-43e5-b16f-e3e8bc1eea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)  # This saves both the model and the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3304777-cba8-4154-9d7a-5d223f999457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model from Google Drive to use it in Google Colab\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_dir = \"/content/drive/My Drive/Models/t5-summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd5e11-ac82-4261-b684-d2a2e60e92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r model.zip {model_dir}\n",
    "from google.colab import files\n",
    "files.download(\"model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "eaaae912-f2de-4068-b2f7-7b8e8a482988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Path to the final saved model\n",
    "model_dir = \"C:\\\\Users\\\\OMEN\\\\BrevityBot\\\\t5-summarization-final\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "43c22f9c-6ec5-4636-9e2d-8f80d0df8389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial institutions were looking for ways to automate the adoption of conversational AI as part of a digital transformation initiative.\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Many financial institutions started building conversational AI, prior to the Covid19\n",
    "pandemic, as part of a digital transformation initiative. These initial solutions\n",
    "were high profile, highly personalized virtual assistants — like the Erica chatbot\n",
    "from Bank of America. As the pandemic hit, the need changed as contact centers were\n",
    "under increased pressures. As Cathal McGloin of ServisBOT explains in “how it started,\n",
    "and how it is going,” financial institutions were looking for ways to automate\n",
    "solutions to help get back to “normal” levels of customer service. This resulted\n",
    "in a change from the “future of conversational AI” to a real tactical assistant\n",
    "that can help in customer service. Haritha Dev of Wells Fargo, saw a similar trend.\n",
    "Banks were originally looking to conversational AI as part of digital transformation\n",
    "to keep up with the times. However, with the pandemic, it has been more about\n",
    "customer retention and customer satisfaction. In addition, new use cases came about\n",
    "as a result of Covid-19 that accelerated adoption of conversational AI. As Vinita\n",
    "Kumar of Deloitte points out, banks were dealing with an influx of calls about new\n",
    "concerns, like questions around the Paycheck Protection Program (PPP) loans. This\n",
    "resulted in an increase in volume, without enough agents to assist customers, and\n",
    "tipped the scale to incorporate conversational AI. When choosing initial use cases\n",
    "to support, financial institutions often start with high volume, low complexity\n",
    "tasks. For example, password resets, checking account balances, or checking the\n",
    "status of a transaction, as Vinita points out. From there, the use cases can evolve\n",
    "as the banks get more mature in developing conversational AI, and as the customers\n",
    "become more engaged with the solutions. Cathal indicates another good way for banks\n",
    "to start is looking at use cases that are a pain point, and also do not require a\n",
    "lot of IT support. Some financial institutions may have a multi-year technology\n",
    "roadmap, which can make it harder to get a new service started. A simple chatbot\n",
    "for document collection in an onboarding process can result in high engagement,\n",
    "and a high return on investment. For example, Cathal has a banking customer that\n",
    "implemented a chatbot to capture a driver’s license to be used in the verification\n",
    "process of adding an additional user to an account — it has over 85% engagement\n",
    "with high satisfaction. An interesting use case Haritha discovered involved\n",
    "educating customers on financial matters. People feel more comfortable asking a\n",
    "chatbot what might be considered a “dumb” question, as the chatbot is less judgmental.\n",
    "Users can be more ambiguous with their questions as well, not knowing the right\n",
    "words to use, as chatbot can help narrow things down.\n",
    "\"\"\"\n",
    "\n",
    "inputs = [\"summarize: \" + text]\n",
    "inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=64, num_beams=4)\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d85857-5d3e-4ed5-a701-2b1af715fd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
